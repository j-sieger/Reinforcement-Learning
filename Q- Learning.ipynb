{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"Q- Learning.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"LxIoaPe9-Sij","colab_type":"code","colab":{}},"source":["import numpy as np\n","import gym\n","import random\n","\n","env = gym.make(\"FrozenLake-v0\")\n","# env.render()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SQNZxjej-Sjq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"52c5d3af-1247-4357-88d0-6eea52e4779b","executionInfo":{"status":"ok","timestamp":1566643718894,"user_tz":-330,"elapsed":1395,"user":{"displayName":"jani basha","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mABIbrZBnv5a4a9GpkcwlNGArEFkk1cN2d92Qzo1Q=s64","userId":"01940010228818985276"}}},"source":["action_size = env.action_space.n\n","print(\"Action size \", action_size)\n","\n","state_size = env.observation_space.n\n","print(\"State size \", state_size)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Action size  4\n","State size  16\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DX8olJsd-Skf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":311},"outputId":"8b7937cb-5410-4b93-8e01-dddecd355bbc","executionInfo":{"status":"ok","timestamp":1566643720082,"user_tz":-330,"elapsed":1316,"user":{"displayName":"jani basha","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mABIbrZBnv5a4a9GpkcwlNGArEFkk1cN2d92Qzo1Q=s64","userId":"01940010228818985276"}}},"source":["qtable = np.zeros((state_size, action_size))\n","print(qtable)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["[[0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"l-6PiZF_-Sk_","colab_type":"code","colab":{}},"source":["total_episodes = 50000 # Total episodes\n","total_test_episodes = 100 # Total test episodes\n","max_steps = 99 # Max steps per episode\n","\n","learning_rate = 0.5 # Learning rate\n","gamma = 0.618 # Discounting rate\n","\n","# Exploration parameters\n","epsilon = 1.0 # Exploration rate\n","max_epsilon = 1.0 # Exploration probability at start\n","min_epsilon = 0.01 # Minimum exploration probability \n","decay_rate = 0.01 # Exponential decay rate for exploration prob"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A-G1IfhR-Slb","colab_type":"code","colab":{}},"source":["# 2 For life or until learning is stopped\n","for episode in range(total_episodes):\n","# Reset the environment\n","    state = env.reset()\n","    step = 0\n","    done = False\n","\n","    for step in range(max_steps):\n","# 3. Choose an action a in the current world state (s)\n","## First we randomize a number\n","        exp_exp_tradeoff = random.uniform(0,1)\n","\n","## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n","        if exp_exp_tradeoff > epsilon:\n","            action = np.argmax(qtable[state,:])\n","\n","# Else doing a random choice --> exploration\n","        else:\n","            action = env.action_space.sample()\n","\n","# Take the action (a) and observe the outcome state(s') and reward (r)\n","        new_state, reward, done, info = env.step(action)\n","\n","# Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n","        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * \n","        np.max(qtable[new_state, :]) - qtable[state, action])\n","\n","# Our new state is state\n","        state = new_state\n","\n","# If done : finish episode\n","        if done == True: \n","            break\n","\n","# Reduce epsilon (because we need less and less exploration)\n","epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-wluj8OR-SmB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"df6fc063-d6ba-486c-8613-d33161befc72","executionInfo":{"status":"ok","timestamp":1566643892631,"user_tz":-330,"elapsed":10273,"user":{"displayName":"jani basha","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mABIbrZBnv5a4a9GpkcwlNGArEFkk1cN2d92Qzo1Q=s64","userId":"01940010228818985276"}}},"source":["env.reset()\n","rewards = []\n","\n","for episode in range(total_test_episodes):\n","   state = env.reset()\n","   step = 0\n","   done = False\n","   total_rewards = 0\n","   #print(\"****************************************************\")\n","   #print(\"EPISODE \", episode)\n","\n","   for step in range(max_steps):\n","      # UNCOMMENT IT IF YOU WANT TO SEE OUR AGENT PLAYING\n","      #env.render()\n","      # Take the action (index) that have the maximum expected future reward given that state\n","      action = np.argmax(qtable[state,:])\n","\n","      new_state, reward, done, info = env.step(action)\n","\n","      total_rewards += reward\n","\n","      if done:\n","         rewards.append(total_rewards)\n","         #print (\"Score\", total_rewards)\n","         break\n","      state = new_state\n","env.close()\n","print (\"Score over time: \" + str(sum(rewards)/total_test_episodes))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Score over time: 0.03\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"12x5Mt-lJmdm","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}